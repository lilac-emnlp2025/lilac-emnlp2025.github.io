@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}








%%%%%%%%%%%%%%%%%%%%%%%% Custom Citations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





@inproceedings{1_lilac,
  title={LILaC: Late Interacting in Layered Component Graph for Open-domain Multimodal Multihop Retrieval},
  author={Yun, Joohyung and Lee, Doyup and Han, Wook-Shin},
  year={2025},
  organization={Association for Computational Linguistics}
}

@article{3_visrag,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@article{4_colpali,
  title={Colpali: Efficient document retrieval with vision language models},
  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C{\'e}line and Colombo, Pierre},
  journal={arXiv preprint arXiv:2407.01449},
  year={2024}
}

@inproceedings{5_m3docvqa,
  title={M3DocVQA: Multi-modal Multi-page Multi-document Understanding},
  author={Cho, Jaemin and Mahata, Debanjan and Irsoy, Ozan and He, Yujie and Bansal, Mohit},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6178--6188},
  year={2025}
}

@inproceedings{6_densexretrieval,
  title={Dense x retrieval: What retrieval granularity should we use?},
  author={Chen, Tong and Wang, Hongwei and Chen, Sihao and Yu, Wenhao and Ma, Kaixin and Zhao, Xinran and Zhang, Hongming and Yu, Dong},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={15159--15177},
  year={2024}
}

@inproceedings{7_mixofgran,
  title={Mix-of-granularity: Optimize the chunking granularity for retrieval-augmented generation},
  author={Zhong, Zijie and Liu, Hanwen and Cui, Xiaoya and Zhang, Xiaofan and Qin, Zengchang},
  booktitle={Proceedings of the 31st International Conference on Computational Linguistics},
  pages={5756--5774},
  year={2025}
}


@inproceedings{8_raptor,
  title={Raptor: Recursive abstractive processing for tree-organized retrieval},
  author={Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{9_ircot,
  title={Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions},
  author={Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 61st annual meeting of the association for computational linguistics (volume 1: long papers)},
  pages={10014--10037},
  year={2023}
}

@article{10_selfrag,
  title={Self-rag: Learning to retrieve, generate, and critique through self-reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  year={2024},
  publisher={ICLR}
}

@inproceedings{11_react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik R and Cao, Yuan},
  booktitle={The eleventh international conference on learning representations},
  year={2022}
}

@article{13_reasonpath,
  title={Learning to retrieve reasoning paths over wikipedia graph for question answering},
  author={Asai, Akari and Hashimoto, Kazuma and Hajishirzi, Hannaneh and Socher, Richard and Xiong, Caiming},
  journal={arXiv preprint arXiv:1911.10470},
  year={2019}
}

@inproceedings{16_mmmulhop,
  title={Enhancing multi-modal multi-hop question answering via structured knowledge and unified retrieval-generation},
  author={Yang, Qian and Chen, Qian and Wang, Wen and Hu, Baotian and Zhang, Min},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={5223--5234},
  year={2023}
}

@article{17_unifiedprese,
  title={Unified language representation for question answering over text, tables, and images},
  author={Yu, Bowen and Fu, Cheng and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2306.16762},
  year={2023}
}

@inproceedings{18_unifying,
  title={Unifying text, tables, and images for multimodal question answering},
  author={Luo, Haohao and Shen, Ying and Deng, Yang},
  year={2023},
  organization={Association for Computational Linguistics}
}

@inproceedings{19_helios,
  title={Helios: Harmonizing early fusion, late fusion, and llm reasoning for multi-granular table-text retrieval},
  author={Park, Sungho and Yun, Joohyung and Lee, Jongwuk and Han, Wook-Shin},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={32424--32444},
  year={2025}
}

@article{20_surveymrag,
  title={A survey of multimodal retrieval-augmented generation},
  author={Mei, Lang and Mo, Siyu and Yang, Zhihan and Chen, Chong},
  journal={arXiv preprint arXiv:2504.08748},
  year={2025}
}

@article{21_beyondtext,
  title={Beyond text: Optimizing rag with multimodal inputs for industrial applications},
  author={Riedler, Monica and Langer, Stefan},
  journal={arXiv preprint arXiv:2410.21943},
  year={2024}
}

@inproceedings{22_colbert,
  title={Colbert: Efficient and effective passage search via contextualized late interaction over bert},
  author={Khattab, Omar and Zaharia, Matei},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={39--48},
  year={2020}
}

@inproceedings{23_colbertv2,
  title={Colbertv2: Effective and efficient retrieval via lightweight late interaction},
  author={Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={3715--3734},
  year={2022}
}

@inproceedings{24_hierargraph,
  title={Hierarchical graph network for multi-hop question answering},
  author={Fang, Yuwei and Sun, Siqi and Gan, Zhe and Pillai, Rohit and Wang, Shuohang and Liu, Jingjing},
  booktitle={Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP)},
  pages={8823--8838},
  year={2020}
}

@article{26_correcrag,
  title={Corrective retrieval augmented generation},
  author={Yan, Shi-Qi and Gu, Jia-Chen and Zhu, Yun and Ling, Zhen-Hua},
  year={2024}
}

@inproceedings{27_mmapg,
  title={Mmapg: A training-free framework for multimodal multi-hop question answering via adaptive planning graphs},
  author={Hu, Yiheng and Wang, Xiaoyang and Liu, Qing and Xu, Xiwei and Fu, Qian and Zhang, Wenjie and Zhu, Liming},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  pages={14195--14211},
  year={2025}
}

@inproceedings{28_unirag,
  title={UniRAG: A Unified RAG Framework for Knowledge-Intensive Queries with Decomposition, Break-Down Reasoning, and Iterative Rewriting},
  author={Kim, Gun Il and Kim, Jong Wook and Jang, Beakcheol},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2025},
  pages={18795--18810},
  year={2025}
}

@article{29_prism,
  title={PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering},
  author={Nahid, Md Mahadi Hasan and Rafiei, Davood},
  journal={arXiv preprint arXiv:2510.14278},
  year={2025}
}

@article{30_mirage,
  title={MIRAGE: Scaling Test-Time Inference with Parallel Graph-Retrieval-Augmented Reasoning Chains},
  author={Wei, Kaiwen and Shan, Rui and Zou, Dongsheng and Yang, Jianzhong and Zhao, Bi and Zhu, Junnan and Zhong, Jiang},
  journal={arXiv preprint arXiv:2508.18260},
  year={2025}
}

@article{31_agentg,
  title={Agent-G: An Agentic Framework for Graph Retrieval Augmented Generation},
  author={Lee, Meng-Chieh and Zhu, Qi and Mavromatis, Costas and Han, Zhen and Adeshina, Soji and Ioannidis, Vassilis N and Rangwala, Huzefa and Faloutsos, Christos}
}

@inproceedings{33_docreact,
  title={Doc-React: Multi-page Heterogeneous Document Question-answering},
  author={Wu, Junda and Xia, Yu and Yu, Tong and Chen, Xiang and Harsha, Sai Sree and Maharaj, Akash V and Zhang, Ruiyi and Bursztyn, Victor and Kim, Sungchul and Rossi, Ryan A and others},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={67--78},
  year={2025}
}

@inproceedings{34_docagent,
  title={Docagent: An agentic framework for multi-modal long-context document understanding},
  author={Sun, Li and He, Liu and Jia, Shuyue and He, Yangfan and You, Chenyu},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  pages={17712--17727},
  year={2025}
}

@inproceedings{35_mara,
  title={MARA: A Multimodal Adaptive Retrieval-Augmented Framework for Document Question Answering},
  author={Wu, Hui and Zhai, Haoquan and Li, Yuchen and Cai, Hengyi and Zhang, Peirong and Zhang, Yidan and Wang, Lei and Wang, Chunle and Hou, Yingyan and Wang, Shuaiqiang and others},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  pages={4329--4338},
  year={2025}
}

@article{solar,
  title={Unified language representation for question answering over text, tables, and images},
  author={Yu, Bowen and Fu, Cheng and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2306.16762},
  year={2023}
}

@inproceedings{unimmqa,
  title={Unifying text, tables, and images for multimodal question answering},
  author={Luo, Haohao and Shen, Ying and Deng, Yang},
  year={2023},
  organization={Association for Computational Linguistics}
}

@article{tog,
  title={Think-on-graph: Deep and responsible reasoning of large language model on knowledge graph},
  author={Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Ni, Lionel M and Shum, Heung-Yeung and Guo, Jian},
  journal={arXiv preprint arXiv:2307.07697},
  year={2023}
}

@article{rog,
  title={Reasoning on graphs: Faithful and interpretable large language model reasoning},
  author={Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
  journal={arXiv preprint arXiv:2310.01061},
  year={2023}
}

@article{gog,
  title={Generate-on-graph: Treat llm as both agent and kg in incomplete knowledge graph question answering},
  author={Xu, Yao and He, Shizhu and Chen, Jiabei and Wang, Zihao and Song, Yangqiu and Tong, Hanghang and Liu, Guang and Liu, Kang and Zhao, Jun},
  journal={arXiv preprint arXiv:2404.14741},
  year={2024}
}

@article{multimodalqa,
  title={Multimodalqa: Complex question answering over text, tables and images},
  author={Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan},
  journal={arXiv preprint arXiv:2104.06039},
  year={2021}
}

@inproceedings{webqa,
  title={Webqa: Multihop and multimodal qa},
  author={Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16495--16504},
  year={2022}
}

@inproceedings{mmcoqa,
  title={Mmcoqa: Conversational question answering over text, tables, and images},
  author={Li, Yongqi and Li, Wenjie and Nie, Liqiang},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={4220--4231},
  year={2022}
}

@article{m3docrag,
  title={M3docrag: Multi-modal retrieval is what you need for multi-page multi-document understanding},
  author={Cho, Jaemin and Mahata, Debanjan and Irsoy, Ozan and He, Yujie and Bansal, Mohit},
  journal={arXiv preprint arXiv:2411.04952},
  year={2024}
}

@article{nvembedv2,
  title={Nv-embed: Improved techniques for training llms as generalist embedding models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@article{mmembed,
  title={Mm-embed: Universal multimodal retrieval with multimodal llms},
  author={Lin, Sheng-Chieh and Lee, Chankyu and Shoeybi, Mohammad and Lin, Jimmy and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2411.02571},
  year={2024}
}

@misc{gpt5,
  title         = {OpenAI GPT-5 System Card},
  author        = {Singh, Aaditya and others},
  year          = {2025},
  month         = dec,
  eprint        = {2601.03267},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL},
  note          = {Released alongside GPT-5 (Aug 2025). Accessed 2026-01-27.}
}