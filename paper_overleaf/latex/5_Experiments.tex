\section{Experiments} 
\label{sec:exp}
\vspace{-2mm}
\subsection{Experimental Setups}


\begin{table*}[t]
  \centering
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.2}
  \scriptsize

  % ──────────────────────────────────────────────────────────────────
  \begin{tabular}{@{}l|c|cc|cc|cc|cc|cc@{}}
    \toprule
    \multirow{3}{*}{\textbf{Algorithm}}
        & \multirow{3}{*}{\textbf{Embedder Type}}
        & \multicolumn{2}{c|}{\textbf{\texttt{MP-DocVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{SlideVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{InfoVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{MultimodalQA}}}
        & \multicolumn{2}{c@{}}{\textbf{\texttt{MMCoQA}}} \\[-1.5pt]
    \cmidrule(lr){3-12}
        &        & R@3 & MRR@10
                 & R@3 & MRR@10
                 & R@3 & MRR@10
                 & R@3 & MRR@10
                 & R@3 & MRR@10 \\
    \midrule
    % ===================== Baselines ===============================
    \texttt{NV-Embed-v2}  & Text 
                 & 67.85 & 61.91 
                 & 88.49 & 79.55 
                 & 86.21 & 80.86
                 & 60.19 & 67.86 
                 & 46.16 & 41.45 \\
    \midrule
    % ---------------- VisRAG & ColPali (merged Image) --------------
    \texttt{VisRAG-Ret} & \multirow{2}{*}{Image} 
                 & \cellcolor{orange!20}{83.25} & \cellcolor{orange!20}{75.55}
                 & \cellcolor{orange!20}{91.55} & \cellcolor{orange!20}{84.30}
                 & \cellcolor{orange!20}{92.76} & \cellcolor{orange!20}{86.22}
                 & 50.08 & 55.08
                 & 27.63 & 23.75 \\[-1pt]
    \texttt{ColPali}      &  
                 & 80.71 & 74.86 
                 & 89.39 & 81.55 
                 & 88.30 & 82.76
                 & 58.73 & 65.05 
                 & 36.24 & 32.33 \\
    \midrule
    % ---------------------- DraGO variants -------------------------
    \textbf{\texttt{LILaC} (w/ \texttt{mmE5})} & \multirow{3}{*}{Multimodal}
                 & 61.25 & 55.30 
                 & 77.52 & 68.80 
                 & 75.09 & 69.86
                 & 54.79 & 59.02 
                 & 48.88 & 40.30 \\[-1pt]
    \textbf{\texttt{LILaC} (w/ \texttt{UniME})} &  
                 & 77.83 & 71.42 
                 & 84.35 & 77.93 
                 & 82.28 & 75.27
                 & 58.52 & 61.44 
                 & 49.63 & 42.97 \\[-1pt]
    \textbf{\texttt{LILaC} (w/ \texttt{MM-Embed})} &  
                 & \textbf{83.59} & \textbf{78.75} 
                 & \textbf{92.81} & \textbf{84.43} 
                 & \textbf{93.17} & \textbf{86.83}
                 & \textbf{69.07} & \textbf{75.28} 
                 & \textbf{55.80} & \textbf{50.77} \\
    \bottomrule
  \end{tabular}
  \vspace{-1mm}
    \caption{
    \updated{Retrieval accuracy (Recall@3 (\textit{R@3}) and \textit{MRR@10}) of \texttt{LILaC} and its competitors on five benchmarks.  
  The best score in each column is in \textbf{bold}.  
  The in-domain fine-tuned settings are colored in \colorbox{orange!20}{orange}.}
  }
  \vspace{-3mm}
  \label{tab:retrieval_accuracy}
\end{table*}


\begin{table*}[t]
  \centering
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.2}
  \scriptsize

  % ──────────────────────────────────────────────────────────────────
  \begin{tabular}{@{}l|c|cc|cc|cc|cc|cc@{}}
    \toprule
    \multirow{3}{*}{\textbf{Algorithm}}
        & \multirow{3}{*}{\textbf{MLLM}}
        & \multicolumn{2}{c|}{\textbf{\texttt{MP-DocVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{SlideVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{InfoVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{MultimodalQA}}}
        & \multicolumn{2}{c}{\textbf{\texttt{MMCoQA}}} \\[-1.5pt]
        
    \cmidrule(lr){3-12}
            &    & EM & F1
                 & EM & F1
                 & EM & F1
                 & EM & F1
                 & EM & F1 \\
    \midrule

    \texttt{NV-Embed-v2}  
                 & \texttt{Qwen2.5-VL 7B} 
                 & 56.51 & 63.16 
                 & 53.77 & 64.41 
                 & 60.72 & 63.40 
                 & 37.23 & 43.85 
                 & 28.05 & 34.67 \\[-1pt]
    \midrule
    \texttt{VisRAG-Ret}
                 & \texttt{MiniCPM V2.6}
                 & \cellcolor{orange!20}54.31 & \cellcolor{orange!20}68.86 
                 & \cellcolor{orange!20}43.88 & \cellcolor{orange!20}62.37 
                 & \cellcolor{orange!20}50.83 & \cellcolor{orange!20}57.55
                 & 28.18 & 34.01 
                 & 21.51 & 27.87 \\[-1pt]
    \texttt{VisRAG-Ret}  & \texttt{Qwen2.5-VL 7B} 
                 & \cellcolor{orange!20}65.34 & \cellcolor{orange!20}72.24 
                 & \cellcolor{orange!20}55.03 & \cellcolor{orange!20}66.13 
                 & \cellcolor{orange!20}60.16 & \cellcolor{orange!20}61.93
                 & 22.24 & 25.55 
                 & 16.69 & 20.90 \\[-1pt]
    \texttt{ColPali}      
                 & \texttt{Qwen2.5-VL 7B} 
                 & 64.46 & 71.16 
                 & 53.77 & 64.54 
                 & 58.07 & 60.38 
                 & 23.59 & 27.37 
                 & 18.07 & 22.30 \\[-1pt]
    \midrule
    
    % \textbf{Ours (w/ QQMM)}  & Multimodal 
    %              & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$ & $\ldots$
    %              & 40.96 & 47.34 & 34.42 & 40.95 \\[-1pt]

    \textbf{\texttt{LILaC} (w/ \texttt{mmE5})}  
                 & \texttt{Qwen2.5-VL 7B} 
                 & 52.96 & 59.53 
                 & 50.89 & 59.07 
                 & 50.12 & 53.12
                 & 40.72 & 47.46 
                 & 33.90 & 40.38 \\[-1pt]

    \textbf{\texttt{LILaC} (w/ \texttt{UniME})} 
                 & \texttt{Qwen2.5-VL 7B} 
                 & 62.43 & 69.40 
                 & 53.05 & 62.89 
                 & 53.39 & 56.86
                 & 43.42 & 49.72 
                 & 33.39 & 40.12 \\[-1pt]
                 
    \textbf{\texttt{LILaC} (w/ \texttt{MM-Embed})} 
                 & \texttt{Qwen2.5-VL 7B} 
                 & \textbf{65.48} & \textbf{72.42}
                 & \textbf{55.57} & \textbf{66.32} 
                 & \textbf{60.91} & \textbf{62.87}
                 & \textbf{44.57} & \textbf{51.97} 
                 & \textbf{36.31} & \textbf{43.22} \\[-1pt]
    \bottomrule
  \end{tabular}
  \vspace{-1mm}
    \caption{
    \updated{End-to-end accuracy (EM and F1) of \texttt{LILaC} and its competitors for the 5 benchmarks.  
  The best score in each column is in \textbf{bold}.
  Generation results corresponding to in-domain fine-tuned settings are colored in \colorbox{orange!20}{orange}.}
  }
  \label{tab:end2end_accuracy}
  \vspace{-3mm}
\end{table*}





















\quad 
\textbf{Datasets \& Evaluation Metrics. }
We evaluate on total five benchmarks.
Three are \texttt{VisRAG}-extended open-domain VQA datasets—\texttt{MP-DocVQA}~\cite{mpdocvqa} (industrial documents), \texttt{SlideVQA}~\cite{slidevqa}(presentation slides with multi-hop queries), and \texttt{InfoVQA}~\cite{infovqa} (infographics).
For a realistic webpage retrieval setting, we extend multimodal QA benchmarks (\texttt{MultimodalQA}~\cite{multimodalqa}, \texttt{MMCoQA}~\cite{mmcoqa}) using \texttt{M3DocRAG}'s methodology~\cite{m3docrag}. 
Specifically, we reconstruct webpages from URLs annotated in each component label. 
\texttt{MultimodalQA} comprises 3,235 webpages, each averaging approximately 37 components, corresponding to about 12 PDF pages.
\texttt{MMCoQA} comprises 453 webpages, each averaging approximately 32 components, 11 PDF pages.


Following \texttt{VisRAG}, we evaluate retrieval using Mean Reciprocal Rank at 10 (MRR@10). 
Additionally, we include Recall@3 to assess whether the retrieval component successfully captures relevant information within the top three components, aligning with \texttt{VisRAG}'s experimental design that inputs three components to the generation model.
Further details are explained in \textsection~\ref{sec:implementation_details}.


\textbf{Compared Methods.} 
We employ two SOTA methods of VisRAG approaches - \texttt{VisRAG}, which directly encodes document images via VLMs~\cite{visrag}, and \texttt{ColPali}, which employs late-interaction multi-vector embeddings from document images~\cite{colpali}.
We additionally compare with \texttt{NV-Embed-v2}, a SOTA TextRAG method reported by \texttt{VisRAG}. It utilizes a 7.85B model for embedding textualized components.



\textbf{Applied Multimodal Embedding Models. }
We use three multimodal embedders: \texttt{MM-Embed}~\cite{mmembed}, \texttt{UniME}~\cite{unime} and \texttt{mmE5}~\cite{mme5}.
Details about the embedding models can be further found in \textsection~\ref{sec:appendix_model_details}.














\subsection{Retrieval Accuracy Comparison}
\label{sec:retrieval_accuracy_comparison}

\begin{table*}[t]
  \centering
  \setlength{\tabcolsep}{4pt}
  \renewcommand{\arraystretch}{1.2}
  \scriptsize

  % ──────────────────────────────────────────────────────────────────
  \begin{tabular}{@{}l|l|cc|cc|cc|cc|cc@{}}
    \toprule
    \multirow{3}{*}{\textbf{Embedder Model}}
        & \multirow{3}{*}{\textbf{Variant}}
        & \multicolumn{2}{c|}{\textbf{\texttt{MP-DocVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{SlideVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{InfoVQA}}}
        & \multicolumn{2}{c|}{\textbf{\texttt{MultimodalQA}}}
        & \multicolumn{2}{c }{\textbf{\texttt{MMCoQA}}} \\[-1.5pt]
    \cmidrule(lr){3-12}
        &   & R@3 & MRR@10
            & R@3 & MRR@10
            & R@3 & MRR@10
            & R@3 & MRR@10
            & R@3 & MRR@10 \\
    \midrule

    % % =====================  (b)  ===================================
                 
    % Screenshot kNN & Multimodal 
    %              & 20.30 & 18.09 
    %              & 63.31 & 54.02 
    %              & 56.55 & 48.02
    %              & 19.71 & 19.96 
    %              & 16.78 & 14.46 \\[-1pt]
                 
    
    \multirow{3}{*}{\texttt{mmE5}} 
                & \texttt{LILaC (w/o LCG \& QD)}
                 & 48.90 & 43.97 
                 & 75.91 & 68.13 
                 & 65.60 & 58.55
                 & 42.99 & 46.92 
                 & 41.22 & 34.51 \\[-1pt]
                 
                 & \texttt{LILaC (w/o QD)}
                 & 60.81 & 55.02 
                 & 74.14 & 67.58 
                 & 67.70 & 60.01
                 & 45.15 & 51.12
                 & 44.18 & 36.62
                 \\[-1pt]
                 
                & \textbf{\texttt{LILaC}} 
                 & \textbf{61.25} & \textbf{55.35}
                 & \textbf{76.80} & \textbf{68.99}
                 & \textbf{68.91} & \textbf{61.18}
                 & \textbf{54.78} & \textbf{59.32}
                 & \textbf{48.54} & \textbf{40.22} \\[-1pt]

    % \midrule

    % \rowcolor{blue!10}
    % \multicolumn{12}{c}{\textbf{\texttt{UniME}}} \\

    \midrule
                 
    % Screenshot kNN & Multimodal 
    %              & 19.46 & 16.91 
    %              & 68.71 & 59.23 
    %              & 69.64 & 62.20
    %              & 28.63 & 30.00 
    %              & 25.83 & 23.03 \\[-1pt]
                 
    \multirow{3}{*}{\texttt{UniME}} 
                 & \texttt{LILaC (w/o LCG \& QD)}
                 & 52.12 & 45.31 
                 & 81.47 & 71.22 
                 & 83.57 & 77.07
                 & 47.68 & 49.06 
                 & 45.78 & 38.41 \\[-1pt]
                 
                 & \texttt{LILaC (w/o QD)}
                 & 77.83 & 71.27 
                 & 83.45 & 75.70 
                 & 85.11 & 78.01
                 & 52.18 & 54.01
                 & 47.11 & 39.85
                 \\[-1pt]
                 
                & \textbf{\texttt{LILaC}} 
                 & \textbf{77.83} & \textbf{71.39}
                 & \textbf{84.35} & \textbf{77.93}
                 & \textbf{85.53} & \textbf{78.81}
                 & \textbf{58.43} & \textbf{61.32}
                 & \textbf{49.45} & \textbf{42.91} \\[-1pt]

    \midrule

    % \rowcolor{blue!10}
    % \multicolumn{12}{c}{\textbf{\texttt{MM-Embed}}} \\

    % \midrule
                 
    % Screenshot kNN & Multimodal 
    %              & 47.21 & 42.01 
    %              & 78.78 & 70.29 
    %              & 57.38 & 51.80
    %              & 45.78 & 49.14 
    %              & 28.03 & 24.15 \\[-1pt]
                 
    \multirow{3}{*}{\texttt{MM-Embed}} 
                 & \texttt{LILaC (w/o LCG \& QD)}
                 & 75.80 & 69.09 
                 & 92.80 & 82.19 
                 & 90.39 & 83.71
                 & 61.10 & 67.35 
                 & 47.94 & 43.75 \\
                 
                 & \texttt{LILaC (w/o QD)}
                 & 82.23 & 77.75 
                 & 92.27 & 83.20 
                 & 92.17 & 85.53
                 & 63.19 & 69.91
                 & 50.18 & 45.59
                 \\[-1pt]
                 
                 & \textbf{\texttt{LILaC}} 
                 & \textbf{83.59} & \textbf{78.75}
                 & \textbf{92.81} & \textbf{84.43}
                 & \textbf{93.17} & \textbf{86.83}
                 & \textbf{69.07} & \textbf{75.28}
                 & \textbf{55.80} & \textbf{50.77} \\

    \bottomrule
  \end{tabular}
  \vspace{-1mm}
  \caption{
    \updated{Ablation study analyzing retrieval accuracy (Recall@3 and MRR@10) of different \texttt{LILaC} variants.
    Best scores per embedder and dataset are highlighted in bold (\texttt{LCG} = Layered Component Graph, \texttt{QD} = Query Decomposition).
    }
  }
  \label{tab:ablation_study}
  \vspace{-3mm}
\end{table*}



We evaluated retrieval accuracies using Recall@3 (R@3) and MRR@10 across five benchmarks. 
Table~\ref{tab:retrieval_accuracy} summarizes the retrieval performance of \texttt{LILaC} and competing methods. 
\updated{Our results indicate that \texttt{LILaC} achieves state-of-the-art (SOTA) performance on all five benchmarks.
Notably, \texttt{LILaC} outperforms the previous VisRAG SOTA models, \texttt{VisRAG-Ret} and \texttt{ColPali}, by substantial margins of 14.24\% and 11.62\% in R@3, and 15.75\% and 11.74\% in MRR@10, on average, respectively. 
These performance gains are especially prominent on datasets that inherently require fine-grained and multihop reasoning (\texttt{MultimodalQA} and \texttt{MMCoQA}), where the relative improvements in average Recall@3 reached 60.68\% and 31.49\%, and MRR@10 improved by 59.90\% and 45.92\%, respectively.}


Our analysis highlights two key findings:
(i) TextRAG of \texttt{NV-Embed-v2}, consistently shows the lowest retrieval accuracy on visually-dependent VQA datasets that include plots and charts, highlighting inherent limitations in handling visual modalities.
(ii) VisRAG methods notably struggle in webpage retrieval settings (\texttt{MultimodalQA}, \texttt{MMCoQA}), underperforming even when compared to the text-based \texttt{NV-Embed-v2}.
Specifically, the stronger VisRAG model, \texttt{ColPali}, showed accuracy drops against \texttt{NV-Embed-v2}, with reductions of 10.70\% in Recall@3 and 20.96\% in MRR@10. 
% (iii) Finally, \texttt{LILaC} underperformed VisRAG methods on \texttt{InfoVQA}, achieving R@3 and MRR@10 scores lower by 6.3\% and 4.16\% than \texttt{VisRAG-Ret}, respectively. 
% Our subsequent analysis attributes this specific gap primarily to suboptimal subcomponent detection within image components in \texttt{InfoVQA}, leading to ineffective late interaction. 















\subsection{End-to-end Accuracy Comparison}

% Paragraph 1: Experiment setting
We conducted end-to-end question answering (QA) experiments to analyze the impact of retrieval accuracy on downstream QA performance. 
The retrieved results were directly input into a multimodal LLM generator for answer generation, primarily using the \texttt{Qwen2.5-VL 7B} model~\cite{qwen2_5}. 
We limited the number of retrieved units fed into the generator to 3, consistent with the experimental setup of \texttt{VisRAG}.
We additionally provide the results from \texttt{MiniCPM V2.6} for comprehensive comparison, following the original \texttt{VisRAG} pipeline. 
Applied prompts are detailed in \textsection~\ref{sec:prompt_templates}.


% Paragraph 2: Experiment results
\updated{Table~\ref{tab:end2end_accuracy} shows that \texttt{LILaC} achieves SOTA end-to-end accuracy on every benchmark, with average EM and F1 scores of 52.56\% and 59.36\%, respectively. 
This represents substantial improvements of 18.67\% and 19.62\% compared to the previously best-performing VisRAG setup, \texttt{VisRAG} with \texttt{Qwen2.5-VL}, which scored 44.29\% in EM and 49.62\% in F1.}
Overall, the end-to-end QA accuracy trends closely align with retrieval accuracy. 
However, a notable exception arises.
Interestingly, despite \texttt{LILaC (w/ mmE5)} having approximately 8.97\% lower retrieval accuracy (R@3) compared to \texttt{NV-Embed-v2}, its EM score surpasses \texttt{NV-Embed-v2} by 19.71\%. 
This divergence highlights the significant information loss inherent to TextRAG methods, which convert visual content entirely into text, underscoring the importance of preserving visual modalities for effective QA. 
% Second, \texttt{LILaC (w/ MM-Embed)} fails to attain SOTA on \texttt{MP-DocVQA}, although it scores the highest retrieval accuracies. 










\subsection{Ablation Study}


% Paragraph 1: 어떠한 variant들이 있는지
We performed an ablation study to assess the individual contributions of each key component in our framework to retrieval accuracy.
\updated{
Specifically, we evaluated two variants of \texttt{LILaC} across all three multimodal embedding models.
\texttt{LILaC (w/o QD)} is a variant of \texttt{LILaC} without its query decomposition module, adn thus not incorporating the late interaction score mechanism.
It instead incorporates a two-stage retrieval approach on the layered graph: it first selects the top $b$-nearest neighbor components at the coarse level, and then reranks these components by considering subcomponent-level relevance scores.
\texttt{LILaC (w/o LCG \& QD)} further discards the layered component graph (\texttt{LCG}) structure.
It directly applies a k-nearest neighbor search on individual top-layer components without leveraging finer-grained subcomponents.
}

\updated{
Table~\ref{tab:ablation_study} indicates that incorporating the layered component graph to the simple baseline (\texttt{LILaC (w/o LCG \& QD)}) shows notable average improvements—7.33\% in R@3 and 10.13\% in MRR@10.
Further integrating query decomposition with the late interaction mechanism to \texttt{LILaC (w/o QD)} completes the \texttt{LILaC} algorithm, yielding incremental gains of 3.19\% in R@3 and 4.7\% in MRR@10.
}
While these improvements seem modest, closer inspection reveals significant benefits in datasets requiring complex multihop reasoning, particularly \texttt{MultimodalQA} and \texttt{MMCoQA}. 
Specifically, incorporating \texttt{QD} improves R@3 by an average of 7.40\% and MRR@10 by 10.70\% for these two datasets.
Overall, \texttt{LILaC} is demonstrated to be a generalizable method, evidenced by its consistent performance improvements across all multimodal datasets and embedding models.
This robust trend underscores \texttt{LILaC}'s ability to universally enhance retrieval performance across a variety of multimodal embedding scenarios.






\vspace{-1mm}
\subsection{Algorithm Execution Time}
\vspace{-1mm}





Figure~\ref{fig:algorithm_execution_time} (a) shows the average retrieval and generation times for each algorithm.
\texttt{LILaC} is approximately 20.76\% slower than \texttt{VisRAG}, yet 18.24\% faster than \texttt{ColPali}. 
Despite employing a unigranular retrieval approach, \texttt{ColPali}'s runtime remained slower due to its inherent complexity from multi-vector embedding methods.
Notably, both VisRAG methods had longer generation times compared to ours.
\texttt{VisRAG} required 1.70$\times$, and \texttt{ColPali} 1.15$\times$ times our average generation runtime, primarily because their pixel-heavy image inputs increased MLLM inference times.

Figure~\ref{fig:algorithm_execution_time} (b) presents the detailed runtime breakdown for \texttt{LILaC}, showing a total average runtime of 3,047 ms. 
Remarkably, the late-interaction-based subgraph retrieval step accounts for only about 48 ms (approximately 1.5\% of the total runtime). 
The major performance bottleneck lies in the query decomposition phase, averaging 1,423 ms. 
Since this step relies on advanced reasoning with the computationally heavy \texttt{Qwen2.5 72B} model, future improvements in runtime efficiency could be realized by utilizing lighter models, thus balancing speed and retrieval accuracy more effectively.


\begin{figure}[t]
\centering
\small
% \vspace{-4mm}
\begin{tabular}{@{}c@{}c@{}}
% \noalign{\vskip -0.1mm}
    { \includegraphics[width=.5\columnwidth]{figures/plot_runtime/Average.pdf}} &
    { \includegraphics[width=.5\columnwidth]{figures/plot_breakdown/Average.pdf}} \\
    \noalign{\vskip -1.3mm}
    (a) Average  runtime &
    (b) Breakdown of \texttt{LILaC} \\
    \end{tabular}
    \vspace{-1mm}
\caption{(a) Comparison of average algorithm execution times across different methods, and (b) detailed runtime breakdown of \texttt{LILaC}.}
\label{fig:algorithm_execution_time}
\end{figure}


\vspace{-1mm}
\subsection{Query Decomposition Analysis}
\vspace{-1mm}

\label{sec:exp_query_decomp}
\updated{We evaluate the query decomposition module in isolation and its impact on retrieval accuracy. 
Because benchmarks do not provide gold subqueries, we approximate decomposition quality via the Jaccard similarity between the \emph{predicted} modality set $\widehat{\mathcal{M}}(q)$ (union of modalities assigned to generated subqueries) and the \emph{gold} modality set $\mathcal{M}^\star(q)$ derived from ground-truth components (where $q$ is a query).
Specifically, the score for a query $q$ is $J(q)=\frac{|\widehat{\mathcal{M}}(q)\cap\mathcal{M}^\star(q)|}{|\widehat{\mathcal{M}}(q)\cup\mathcal{M}^\star(q)|}$, and the final accuracy is obtained by averaging over all queries.}

\updated{We compare Qwen2.5 (8B, 72B) and Llama3.1 (7B, 70B) using the prompts of \S\ref{sec:prompt_templates}, keeping all other components fixed. 
In table~\ref{tab:query_decomp_llms}, we report (i) decomposition accuracy, (ii) Recall@3 (R@3), and (iii) decomposition runtime (time (ms)) on \texttt{MultimodalQA} and \texttt{MMCoQA}, as they are the only datasets with $\mathcal{M}^\star(q)$ labeled.
}


\begin{table}[t]
    \centering
    \setlength{\tabcolsep}{4pt}
    \renewcommand{\arraystretch}{1.2}
    \footnotesize
    \begin{tabular}{@{}c|c|c|c|c@{}}
        \toprule
        
        \textbf{LLM} & \textbf{Params} & \textbf{DAcc (\%)} & \textbf{R@3 (\%)} & \textbf{Time (ms)} \\
        
        \midrule
        
        \multirow{2}{*}{Qwen2.5}    & 8B & 63.29 & 59.01 & 258 \\
        \cmidrule(lr){2-5}
        
                                    & 72B & 72.23 & 62.43 & 1849 \\
        \cmidrule(lr){1-5}
        
        \multirow{2}{*}{Llama3.1}   & 7B & 58.11 & 57.44 & 336 \\
        \cmidrule(lr){2-5}
        
                                    & 70B & 66.34 & 61.24 & 1731 \\
                                    
        \bottomrule
    \end{tabular}
    \vspace{-1mm}
    \caption{Query decomposition analysis result on \texttt{MultimodalQA} and \texttt{MMCoQA} datasets.}
    \label{tab:query_decomp_llms}
\end{table}

\updated{
We notice that the LLM-driven decomposition attains reasonable accuracy of 72.23\%, and also that larger models improve both decomposition and retrieval at higher latency. 
Notably, decomposition accuracy and Recall@3 are strongly correlated across model variants (Pearson $\rho{=}0.954$), underscoring that better query decomposition directly benefits retrieval.
}

\vspace{-1mm}
\subsection{Additional Experiments}
\vspace{-1mm}

\updated{
Additional experiments were conducted, but are detailed in the appendix due to space limitations.
These include (i) parameter sensitivity (\textsection~\ref{sec:parameter_sensitivity}), 
(ii) analysis of offline layered component graph construction runtime (\textsection~\ref{sec:lcg_construction_overhead}), and 
(iii) a comparison of retrieval accuracy across different embedder models (\textsection~\ref{sec:embedder_comparison}).
}
