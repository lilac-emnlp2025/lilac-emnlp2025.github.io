

\section{Limitations}

% LLM을 사용하는 알고리즘의 특성 상, LILaC과 비교하였을 때 retrieval time이 오래 걸리는 문제가 있음.
% 또한, 우리는 document가 이미 component 형태로 parsing되어 있다는 것을 가정한다.
% 즉 이러한 parsing 및 preprocessing quality에 따라 우리의 accuracy가 결정된다.
% 향후로는 어떠한 webpage에도 적용 가능한 retrieval 방법론을 개발하고자 한다.
% \textbf{Latency and cost.}
\textsc{\Ours} incurs higher wall-clock latency and API cost than lightweight or fixed-call traversal methods such as \textsc{LILaC}.
Although our cost-aware strategy escalation mitigates unnecessary computation, multi-step orchestration and occasional LLM-based reranking remain as bottlenecks. %, limiting throughput in high-volume or real-time settings.
% \textbf{Dependence on preprocessing.}
Our method assumes that each webpage/document is already parsed into a clean set of multimodal components and that navigational signals are reliably extracted.
% In practice, parsing and preprocessing quality (e.g., component segmentation, table extraction, OCR for images, and link resolution) can vary widely across sources, and errors in this pipeline may propagate to retrieval decisions and reduce accuracy.
% \textbf{Scope of applicability.}
While we focus on URL-annotated corpora with structured navigation and component graphs, truly open-web deployment may involve noisier pages, weaker link signals, dynamic content, and heterogeneous layouts.
An important direction for future work is to develop retrieval pipelines that are robust to diverse webpage structures and can jointly learn or adapt the parsing, graph construction, and retrieval policy so that the approach generalizes more seamlessly to arbitrary web content.




\section{Model Details}
\label{sec:appendix_model_details}


\textbf{(Multimodal) Large language models:}
\squishlist
    \item \texttt{Open-AI ChatGPT5}
\squishend

\noindent \textbf{Text embedders}
\squishlist
    \item \texttt{NV-Embed-v2}: 7.85B parameters
\squishend

\noindent \textbf{Cross-modal embedders:}
\squishlist
    \item \texttt{ColPali}: 3B parameters
    \item \texttt{VisRAG}: 3.43B parameters
\squishend


\noindent \textbf{Multimodal embedders:}
\squishlist
    \item \texttt{MM-Embed}: 8.18B parameters
\squishend






\section{Experimental Details}
\label{appendix:exp_details}




\subsection{Hardware and Software Settings}
\label{appendix:hardware_software_settings}
All experiments were conducted on a Linux server equipped with an Intel Xeon Gold 6230 CPU @ 2.10~GHz, 1~TB of RAM, and four NVIDIA RTX~A6000 GPUs, running Ubuntu~22.04.3~LTS. 


\subsection{Implementation Details}
\label{sec:implementation_details}

Our main hyperparameter is the vector-search shortlist size $k$ used by the Multi-strategy Traverser at both the document-level and component-level stages.
Unless otherwise noted, we set $k{=}30$ for all experiments.


\subsection{Benchmark Details}


\MultimodalQA: We use the extended version of \texttt{MultimodalQA}, following the augmentation procedure introduced in M3DocRAG~\cite{m3docrag}. 
It spans diverse document modalities (text, images, and tables) and is designed to stress multi-hop reasoning over multiple documents. 
The evaluation split contains 2,441 questions grounded in 3235 webpages.

\MMCoQA: We use an extended variant of \texttt{MMCoQA} that moves beyond the original distractor-only setting to evaluate conversational, multi-turn multimodal QA. 
The benchmark consists of coherent dialogue sessions in which later questions depend on earlier context and require aggregating evidence across text, images, and tables. 
It includes 5,753 questions grouped into 1,179 conversations, with a corpus of 218,285 text passages, 10,042 tables, and 57,058 images.

\WebQA:
\WebQA\ is a Wikipedia-based multimodal QA benchmark with 4,966 questions over 7,662 documents.
Because the original answers are often verbose, we rewrite them into concise references using the ChatGPT-5 OpenAI API with the prompt below.



\begin{promptbox}{Answer Concisification Prompt for \WebQA}
You are an assistant that extracts concise answers from an Original Answer.

Task:
Given a Question, its Question Category (Qcate), and an Original Answer, extract a concise version of the answer.

Category hints:
- YesNo: respond only "Yes" or "No" matching the polarity of the Original Answer.
- text: return the minimal noun phrase/name that answers the question.
- choose: return only the chosen option or label.
- number: return the numeric value (and unit if present) without extra words.
- color: return the color term(s) only.
- shape: return the shape descriptor only.
- Others: follow the general concise rules below.

Rules:
- Output ONLY the concise answer text (no extra words, no labels, no punctuation-only output).
- Keep the minimum span that directly answers the Question.
- Prefer a single word when possible.
- If the question asks what object/thing, output the object noun phrase only (e.g., fountain).
- If the question asks for a choice/comparison attribute (e.g., taller or shorter, happy or upset, or similar), output only the chosen option word from the answer (e.g., "taller", "upset").
- If the Original Answer is verbose by repeating or paraphrasing words/phrases already present in the Question, do NOT copy those repeated Question words into the concise answer; extract only the new, directly-question-answering information (If those repeated words are necessary for answering the question, then you may include them).
- Preserve the original casing/pluralization as used in the Original Answer (e.g., "Circles").
- Do not include locations, explanations, or surrounding context unless they are required to uniquely answer the question.

Examples:
# Example 1
Question Category: YesNo
Question: Does a Minnetonka Rhododendron flower have petals in a cup shape?
Original Answer: No, a Minnetonka Rhododendron flower does not have petals in a cup shape.
Concise Answer: No

# Example 2
Question Category: Others
Question: What water-related object is sitting in front of the Torre del Reloj?
Original Answer: A fountain is sitting in front of the Torre del Reloj.
Concise Answer: fountain

# Example 3
Question Category: choose
Question: Is the fence in front of The Glass House in Fulham taller or shorter than a bicycle?
Original Answer: The fence in front of the building is taller than a typical bicycle.
Concise Answer: taller

# Example 4
Question Category: shape
Question: What shape is found 3 times on the front of the Archway in King Charles Street?
Original Answer: Circles may be spotted three times on the face of the Archway on King Charles Street.
Concise Answer: Circles

# Example 5
Question Category: choose
Question: Does the character in the work \"Beslotentuinfeest\" look happy or upset?
Original Answer: The character in the work \"Beslotentuinfeest\" looks upset.
Concise Answer: upset

# Example 6
Question Category: number
Question: How many more skis were used by Anders S\u00f6dergren during the 2010 Olympics than were used by Martin Rulsch during the 2020 Winter Youth Olympics?
Original Answer: Anders S\u00f6dergren used two more skis during the 2010 Olympics.
Concise Answer: two

Inputs:
Question Category (Qcate): {qcate}
Question: {question}
Original Answer: {answer}
Concise Answer:
\end{promptbox}



\section{Prompts Templates of \textsc{\OurFullName}}

% \begin{tcolorbox}
% [breakable, title = Prompt Used to Generate Long Textual Description for Hard Prompt Methods, colback = gray!10, colframe = black, sharp corners, boxrule=0.5mm]
% Describe only the {object} in the image. Focus on its most distinctive features such as shape, color, and material. Write the result in English as a short noun phrase, not a full sentence. Do not mention orientation, position, or surroundings.
% \end{tcolorbox}

% \begin{tcolorbox}
% [breakable, title = Prompt Used to Generate Short Textual Description for Hard Prompt Methods, colback = gray!10, colframe = black, sharp corners, boxrule=0.5mm]
% Describe only the {object} in the image. Write the result in English as a single, highly detailed noun phrase, not a full sentence. Include as many intrinsic and identifying features as possible, such as overall shape, dimensions, proportions, material, surface finish, textures, patterns, colors, edges, rims, openings, and any decorative or structural details. Do not mention orientation, position, or surroundings.
% \end{tcolorbox}






\begin{promptbox}{Orchestrator}
You are a retrieval action-decider assistant.

Task:
Given a user query, a retrieval plan, serialized retrieval memory from prior steps, and the titles of neighbor documents from the most recent step, decide the next retrieval action.

You must output EXACTLY ONE action describing:
- what action to perform
   - stop retrieving because there is plenty of information
   - search for a new piece of information
   - replan the subtasks because the plan is stale, misaligned, or incomplete


# Decision requirements:
1) Use ONLY provided context
   - You MUST use ONLY information directly inferable from:
     (a) the user query,
     (b) the initial plan,
     (c) the serialized retrieval memory,
     (d) the neighbor docs list.
   - Do NOT add facts, assumptions, background knowledge, or outside context.

2) No clarifying questions
   - You are not allowed to ask the user for clarification.
   - Make the best decision using only the given inputs.

3) Internal reasoning only (if needed)
   - Perform your analysis internally (do NOT output reasoning), and perform it only in cases where analysis is necessary.
   - Output JSON only no explanations or extra text.


# Rule Regarding Next Retrieval Subtask: 
  - If the latest retrieval was marked "answerable" OR the previous action used "llm reasoning" for BOTH `document_search_mode` and `component_search_mode`, you should advance to the next unresolved subtask when selecting `next_retrieval_subtask`.
  - Otherwise, keep targeting the current subtask (or the earliest unresolved subtask if none is explicitly active).
  - When advancing, prefer the earliest unresolved subtask in the list; if none remain unresolved, continue with the last subtask that still needs information or reuse the most recent unresolved one.


# Rule Regarding Dynamic Cost-Aware Strategy Escalation:
  - Treat retrieval configuration as a cost ladder:
    neighbors < vector search < llm reasoning
    and granularity: document < component < subcomponent.
  - Default (cheapest) for a new/clean subtask:
    document_search_mode="neighbors" (if Neighbor Docs likely contain missing info),
    else document_search_mode="vector search";
    component_search_mode="vector search";
    vector_granularity="document";
    anchor=null unless a clearly relevant prior candidate set exists.
  - Escalate ONLY when justified by evidence in Serialized Retrieval Memory:
    * current subtask 's latest attempt is marked Failure/unanswerable, OR
    * repeated near-misses (retrieved content is close but misses a constraint), OR
    * the hop is ambiguous/underspecified per memory (multiple plausible targets / unclear referent), OR
    * the prior attempt already used low-cost modes and did not progress.
  - Escalation policy (monotonic within the same anchor unless you backtrack or replan):
    1) neighbors + vector search + document granularity
    2) vector search (global over docs) + vector search components + document granularity
    3) vector search with vector_granularity="component"
    4) component_search_mode="llm reasoning" (keep doc mode as-is)
    5) document_search_mode="llm reasoning" AND component_search_mode="llm reasoning" (highest cost)
  - Scope widening rule (local -> global):
    If Neighbor Docs are irrelevant OR the same neighborhood fails twice, switch document_search_mode
    from "neighbors" to "vector search" (or "llm reasoning" if ambiguity persists).

# Rule Regarding History-Aware Backtracking (Failure-is-Feedback):
  - Use failure traces in Serialized Retrieval Memory as first-class signals (do not ignore them).
  - Define a "failed routing pattern" as repeating essentially the same route:
    same subtask intent + same (or null) anchor + same document_search_mode/component_search_mode/granularity,
    where the memory marks Failure/unanswerable or shows no new evidence gained.
  - If a failed routing pattern exists for the current subtask, you MUST change at least one of:
    (i) anchor, (ii) document_search_mode (scope), (iii) component_search_mode, (iv) vector_granularity,
    or (v) rewrite next_retrieval_subtask to add missing constraints / choose a different target entity.
  - Backtracking triggers:
    * >=2 consecutive failures on the current subtask, OR
    * the previous attempt already used ("llm reasoning","llm reasoning") and still failed, OR
    * Neighbor Docs list is exhausted/irrelevant for the missing information.
  - Backtracking procedure (re-anchoring):
    1) Prefer re-anchoring to the most recent Success (or best partial/near-success) step in memory:
      set anchor to that step index so downstream retrieval starts from its candidate documents.
    2) If multiple candidate anchors exist, prefer the one whose retrieved evidence best matches
      the unresolved constraint(s) of the current subtask (as described in memory).
    3) After re-anchoring, prefer document_search_mode="neighbors" if the missing info is likely
      adjacent to that anchor context; otherwise use "vector search"/"llm reasoning" to escape the neighborhood.
  - Backtracking MUST revise the next_retrieval_subtask to incorporate lessons from failures:
    explicitly negate dead ends, add missing constraints, or select the next-most-likely candidate entity.
  - Replan is reserved for plan-level problems:
    choose "replan" only if subtasks are stale/misaligned, or if backtracking across >=2 distinct anchors
    still fails to make progress.



# Action Fields

(1) "stop"
   - Meaning: Retrieval memory already contains sufficient components to answer the original query.

Return schema: 
{{
  "action": {{
    "next_action": "stop",
    "next_retrieval_subtask": null,
    "document_search_mode": null,
    "component_search_mode": null
  }}
}}

(2) "search"
   - Meaning: More retrieval is needed to answer the original query.
   - Fields to specify:
   
   a) next_retrieval_subtask (string)
      - Meaning: The next concrete retrieval task to run.
      - Must be a short, actionable retrieval prompt (imperative verb + object).
      - Must be consistent with the user query and the initial plan.
      - Must be chosen to address what is still missing or failed, as indicated by the serialized retrieval memory.
      - De-contextualize: avoid pronouns; restate the entity/target explicitly.
      - Generate a retrieval task that tries to solve either the first `Target Subtask to Solve`, or combinations of them. Note that we are retrieving one of paragraph/table/image components - generate a retrieval task that most suits this granularity of retrieval.
      - [IMPORTANT] If the `User Query` is about finding a single entity that meets a certain condition, and `Target Subtask to Solve` indicates retrieving multiple entities then choosing one that meets a certain condition, 
         try using common sense to pick the most likely entity that meets the condition and generate a retrieval task for that entity only.
         * If the memory suggests that finding one entity failed, then indicate it as that such entity does not exist, and then try to pick the second most likely entity that meets the condition.

   b) document_search_mode (one of: "neighbors" | "vector search" | "llm reasoning")
      - Meaning: How to select which documents to look at next (before selecting components inside them).
      - "neighbors":
         Use when the neighbor documents list (given as `# Neighbor Docs`) is likely sufficient for the next_retrieval_subtask.
         Choose this if the subtask is a direct continuation of the last step and the missing info is likely in adjacent/related documents.
      - "vector search":
         Use when neighbor docs (given as `# Neighbor Docs`) are likely insufficient and you need to search across the entire corpus.
         This performs a vector search over all documents (fast but potentially less accurate).
         Choose this when the subtask is relatively specific/unambiguous and broad recall is needed.
      - "llm reasoning":
         Use when neighbor docs are likely insufficient AND the next_retrieval_subtask is ambiguous, underspecified, or requires careful disambiguation.
         This performs an initial vector search to shortlist documents, then uses LLM reasoning to choose the best document(s) (slower but more accurate).

   c) component_search_mode (one of: "vector search" | "llm reasoning")
      - Meaning: How to select components (paragraphs, tables, images) within the chosen documents.
      - "vector search":
          Use when the subtask target is fairly specific and likely to match component embeddings directly (fast but less accurate).
      - "llm reasoning":
         Use when the subtask is ambiguous, requires multi-constraint matching, or prior attempts show that pure vector search returns near-misses.
         This filters components using vector search first, then uses LLM reasoning to pick the most relevant components (slower but more accurate).

   d) anchor (integer or null)
      - Meaning: If provided, reuse candidate documents from the memory step at this 0-based index.
      - How to use:
        * anchor refers to the memory's retrieval steps list index.
        * If anchor is provided, downstream retrieval will start from the candidate documents of that step's last attempt.
      - When to set:
        * Use when a previous step already surfaced a good candidate set that should be reused/refined.
        * Otherwise set to null.

   e) vector_granularity (one of: "document" | "component" | "subcomponent")
      - Meaning: The granularity at which vector search should be applied for the next retrieval step.
      - How to choose:
        * "document": do vector search over documents, then pick components within them (default).
        * "component": do vector search directly over components.
        * "subcomponent": do vector search at a finer granularity (e.g., sentences/snippets) when component-level recall is too coarse.

Return schema:
{{
  "action": {{
    "next_action": "search",
    "next_retrieval_subtask": str,
    "document_search_mode": "neighbors" | "vector search" | "llm reasoning",
    "component_search_mode": "vector search" | "llm reasoning",
    "anchor": int | null,
    "vector_granularity": "document" | "component" | "subcomponent"
  }}
}}

(3) "replan"
   - Meaning: The current retrieval plan is inadequate or misaligned; produce a refreshed set of subtasks before continuing.
   - Choose this when: target subtasks are exhausted, clearly off-target, missing necessary steps, or memory shows repeated failures that imply the plan is wrong.
   - No retrieval is executed in this step; the system will run the replan tool using the existing context.

Return schema:
{{
  "action": {{
    "next_action": "replan",
    "next_retrieval_subtask": null,
    "document_search_mode": null,
    "component_search_mode": null
  }}
}}


Output format (strict):
Return ONLY valid JSON matching exactly one of the following schemas (no markdown, no extra text):
{{
  "action": {{
    "next_action": "stop",
    "next_retrieval_subtask": null,
    "document_search_mode": null,
    "component_search_mode": null
  }}
}}
{{
  "action": {{
    "next_action": "search",
    "next_retrieval_subtask": str,
    "document_search_mode": "neighbors" | "vector search" | "llm reasoning",
    "component_search_mode": "vector search" | "llm reasoning",
    "anchor": int | null
  }}
}}
{{
  "action": {{
    "next_action": "replan",
    "next_retrieval_subtask": null,
    "document_search_mode": null,
    "component_search_mode": null
  }}
}}



Inputs:
# User Query
{query}

# Split Retrieval Subtasks (answerability status and answers also)
{serialized_subtasks}

# Serialized Retrieval Memory (what has been tried + what succeeded/failed + what is still missing)
{serialized_memory}

# Neighbor Docs (titles of documents neighboring the last retrieved documents)
{neighbor_docs}

Output:
\end{promptbox}




\begin{promptbox}{Document-level Traverser}
You are a retrieval selection assistant.

Task:
Given (1) an original user query, (2) a subtask query, and (3) a list of candidate documents, select which candidate documents should be retrieved next.
Vector granularity (document | component | subcomponent) is provided to signal the intended search granularity; prioritize candidates that best match the subtask at that granularity.

Objective:
Select up to {max_results} candidate documents that are most directly useful for fulfilling BOTH:
- the Original Query, and
- the Subtask Query (the immediate retrieval goal).

Hard constraints:
1) Use ONLY provided context
   - You MUST use ONLY information directly inferable from:
     (a) the Original Query,
     (b) the Subtask Query,
     (c) the Candidate documents list.
   - Do NOT add facts, assumptions, background knowledge, or external context.
   - Do NOT introduce any documents that are not in the candidate list.

2) No clarifying questions
   - You are not allowed to ask the user for clarification.
   - Make the best selection using only the given inputs.

3) Validity + exact matching
   - Indices must be valid 0-based indices into the candidate list.
   - Filenames must exactly match a filename from the candidate list.

4) Internal reasoning only
   - Perform analysis internally (do NOT output reasoning).
   - Output JSON only no explanations or extra text.

Scoring rules (for "score"):
- Assign a relevance score in [0.0, 1.0] for each selected document.
- Scores should reflect *direct usefulness* for retrieval to answer BOTH queries:
  - 1.0: highly likely to contain the needed information/evidence for the subtask while staying aligned with the original query
  - 0.5: partially relevant (may help, but incomplete or tangential)
  - 0.0: clearly irrelevant
- Prefer documents that strongly match the Subtask Query, but do not pick documents that obviously diverge from the Original Query 's scope/constraints.

Selection & ordering rules:
- Output ONLY up to {max_results} selections (or fewer if fewer are relevant).
- Do NOT include candidates outside the top selections.
- Sort selections by descending relevance score (highest first).
- If unsure, pick the most likely candidates based on the queries.
- Never return an empty selection unless nothing is relevant at all (i.e., all candidates are clearly irrelevant).

Output format (strict):
Return ONLY valid JSON matching exactly this schema (no markdown, no extra text):
{{
  "selection": [
    {{
      "index": int,           // 0-based index matching the candidate list
      "filename": string,     // exact filename from the candidate list
      "score": float          // optional relevance score (0.0-1.0)
    }}
  ]
}}

Inputs:
# Original Query
{original_query}

# Subtask Query
{subtask_query}

# Vector granularity
{vector_granularity}

# Candidate documents (0-based indices):
{candidates}

# Max results
{max_results}

Output:
\end{promptbox}


\begin{promptbox}{Component-level Traverser}
You are a component selection assistant for retrieval.

Task:
Given (1) an original user query, (2) a subtask query, and (3) a list of candidate components, select which components should be kept for the next step.
Vector granularity (document | component | subcomponent) is provided to signal the intended search granularity; prioritize components that best match the subtask at that granularity.

Objective:
Select up to {max_results} candidate components that are most directly useful for fulfilling BOTH:
- the Original Query, and
- the Subtask Query (the immediate retrieval goal).

Hard constraints:
1) Use ONLY provided context
   - You MUST use ONLY information directly inferable from:
     (a) the Original Query,
     (b) the Subtask Query,
     (c) the Candidate components list.
   - Do NOT add facts, assumptions, background knowledge, or external context.
   - Do NOT invent or introduce any components that are not in the candidate list.

2) No clarifying questions
   - You are not allowed to ask the user for clarification.
   - Make the best selection using only the given inputs.

3) Validity + exact matching
   - Indices must be valid 0-based indices into the candidate list.
   - Filenames and component_ids must exactly match a candidate entry.

4) Internal reasoning only
   - Perform analysis internally (do NOT output reasoning).
   - Output JSON only no explanations or extra text.

Scoring rules (for "score"):
- Assign a relevance score in [0.0, 1.0] for each selected component.
- Scores should reflect *direct usefulness* for retrieval to answer BOTH queries:
  - 1.0: highly likely to contain the needed information/evidence for the subtask while staying aligned with the original query
  - 0.5: partially relevant (may help, but incomplete or tangential)
  - 0.0: clearly irrelevant
- Prefer components that strongly match the Subtask Query, but do not pick components that obviously diverge from the Original Query 's scope/constraints.
- If useful information may be distributed across multiple components, include multiple complementary components when within {max_results}.

Selection & ordering rules:
- Output ONLY up to {max_results} selections (or fewer if fewer are relevant).
- Do NOT include candidates outside the selected set.
- Sort selections by descending relevance score (highest first).
- If unsure, pick the most likely candidates based on the queries.
- Avoid empty outputs unless nothing is relevant at all (i.e., all candidates are clearly irrelevant).

Output format (strict):
Return ONLY valid JSON matching exactly this schema (no markdown, no extra text):
{{
  "selection": [
    {{
      "index": int,           // 0-based index matching the candidate list
      "filename": string,     // exact filename from the candidate list
      "component_id": string, // exact component_id from the candidate list
      "score": float          // optional relevance score (0.0-1.0)
    }}
  ]
}}

Inputs:
# Original Query
{original_query}

# Subtask Query
{subtask_query}

# Vector granularity
{vector_granularity}

# Candidate components (0-based indices):
{candidates}

# Max results
{max_results}

Output:
\end{promptbox}

\begin{promptbox}{Subquery Planner}

You are a retrieval-plan revision assistant.

Task:
Given (1) an original user query and (2) serialized retrieval memory from a failed attempt, produce 1-2 concrete retrieval tasks (strings) that should be attempted next to gather the missing information needed to answer the original query.

Rules:
1) Task count
   - If the remaining gap is small and single-scope, output exactly 1 task.
   - Otherwise output 2 tasks.

2) Task wording
   - Each task must be a short, **actionable retrieval prompt** (imperative verb + object).
   - Do not put reasoning (logical derivation) as a task. Such reasoning should be done along with corresponding retrieval.
   - Keep tasks focused: ideally one clear information need per task.

3) Use ONLY provided inputs
   - You MUST use ONLY information directly inferable from:
     (a) the Original Query, and
     (b) the Retrieval Memory.
   - Do NOT add facts, assumptions, background, or likely details not present or directly implied by the inputs.
   - Do NOT rely on outside knowledge.

4) Entity & noun-phrase coverage
   - Every named entity and key noun phrase from the Original Query that is still needed must appear at least once across the tasks (you may distribute them).
   - Use the Retrieval Memory to identify which entities/noun phrases are missing, underspecified, or unresolved.

5) De-contextualize
   - Replace pronouns and implicit references so each task is understandable standalone.
   - Avoid it/they/this; restate the referenced entity.
   - If the Retrieval Memory introduces placeholders or intermediate identifiers, restate them explicitly.

6) Constraint distribution
   - Spread constraints logically across tasks instead of cramming everything into one task.
   - Prefer separating: identification/disambiguation vs. evidence gathering vs. attribute extraction.

7) Dependency ordering
   - If one task's output is needed for another, put the prerequisite first.
   - Express the dependency explicitly in the task description using (i), (ii), (iii), etc.
   - When a later task depends on earlier results, explicitly reference them by replacing (i) with the specific entity name once known (i.e., keep the "(i)" placeholder until resolved).

Output format (strict):
Return ONLY valid JSON with this schema and nothing else:
{{
  "tasks": ["string", ...]
}}

Examples:



Inputs:
# Original Query
{original_query}

# Retrieval Memory
{memory}

Output:
\end{promptbox}


\begin{promptbox}{Traversal Evaluator}

You are a retrieval evaluation assistant.

Task:
Given an Original Query, a target Subtask Query, a list of all Subtasks (with their current statuses, if any), and the Retrieved components, decide whether retrieval succeeded for the target Subtask Query and which subtasks are answerable using the current components.

Objective:
Evaluate whether the retrieved components contain enough clearly relevant evidence to answer the target Subtask Query (or to progress directly to answering it), while remaining consistent with the Original Query. Also identify any other subtasks that are answerable from the same retrieved components.

Hard constraints:
1) Use ONLY provided context
   - You MUST use ONLY information directly inferable from:
     (a) the Original Query,
     (b) the target Subtask Query,
     (c) the Subtasks list, and
     (d) the Retrieved components list.
   - Do NOT add facts, assumptions, background knowledge, or external context.
   - Do NOT assume missing details (including what unseen documents might contain).

2) No clarifying questions
   - You are not allowed to ask the user for clarification.
   - Make the best determination using only the given inputs.

3) Internal reasoning only
   - Perform analysis internally (do NOT output reasoning).
   - Output JSON only no explanations or extra text.

# Decision rules for `status`:
- Output "answerable" if at least one retrieved component appears:
  (i) clearly relevant to the given `# Subtask Query`, OR
  (ii) directly answers the `# Original query`, if combined with the `# Subtask status`, OR
  (iii) directly answers any of the `# Subtask query` that are marked as "not answerable" or "unknown" in the `# Subtask status` list, , if combined with the `# Subtask status`.
- Otherwise output "not answerable".

# Updated subtasks (independent of target status):
Using the `#Retrieved components` and `# Subtask status`, scan the provided Subtask status list and identify which subtasks are answerable now.
- "updated_subtasks" MUST include ONLY the unanswerable subtasks from the `# Subtask status` that have become answerable using the current Retrieved components + subtask status.
- For each included subtask that are not answerable:
  - If it is answerable using `Retrieved components + subtask status`, then
    - Provide its 1-based index from the provided Subtask status list.
    - Set status = "answerable".
    - Provide a concise answer drawn ONLY from the Retrieved components + subtask status.
  - Else skip it.
- If no subtasks are answerable, output an empty list [].


Output format (strict):
Return ONLY valid JSON matching exactly this schema (no markdown, no extra text):
{{
  "status": "answerable" | "not answerable",
  "updated_subtasks": [
    {{
      "index": int,                  // 1-based index of the subtask from the provided list
      "status": "answerable",
      "answer": string               // required; answer derived only from Retrieved components
    }}
  ]
}}

Inputs:
# Original Query
{original_query}

# Subtask Query
{subtask_query}

# Retrieved components (0-based indices):
{candidates}

# Subtask status (1-based index; include current status if any)
{subtasks}

Output:     
\end{promptbox}


\begin{promptbox}{Reranker}
You are a reranking assistant for retrieval.

Task:
Given a user query and a list of candidate components, select and rank the TOP-{top_k} candidates by how directly useful they are for answering the user query.

Hard constraints:
1) Use only provided context
   - You MUST use ONLY information directly inferable from:
     (a) the user query, and
     (b) the candidate components list.
   - Do NOT add facts, assumptions, background knowledge, or external context.
   - Do NOT complete missing details with parametric knowledge.

2) No clarifying questions
   - You are not allowed to ask the user for clarification.
   - Make the best ranking using only the given inputs.

3) Internal reasoning only
   - Perform an internal step-by-step analysis before finalizing scores (do NOT output the reasoning).
   - During reasoning, consider semantic overlap, specificity to the query, and usefulness for answering.

Scoring rules:
- Assign a relevance score in [0.0, 1.0] to selected candidates.
- Scores should reflect *direct* usefulness for answering the query:
  - 1.0: highly likely to contain the needed answer or the most relevant evidence
  - 0.5: partially relevant or tangentially useful
  - 0.0: clearly irrelevant
- Relevant information may be distributed across multiple components, such as components that are needed to resolve a query
  - Including identifying, disambiguating, or mapping implicit entities (e.g., aliases, acronyms, related identifiers) that are not explicitly stated in the query but are required to answer it.
  - Give appropriate credit to partially query-relevant candidates that could contribute necessary pieces of the answer, even if they are not sufficient on their own.
- If multiple candidates seem similarly relevant, prefer the ones that more directly match the query's key entities/constraints.

Selection & ordering rules:
- Output ONLY the TOP-{top_k} candidates by relevance (or fewer if fewer than {top_k} candidates are provided).
- Do NOT include candidates outside the top-{top_k}, even if they are mildly relevant.
- Sort the output by descending score (highest first).
- Indices are 0-based and must match the candidate list.

Output format (strict):
Return ONLY valid JSON matching exactly this schema (no markdown, no extra text):
{{
  "ranking": [
    {{
      "index": int,           // 0-based index matching the candidate list
      "filename": string,     // exact filename from the candidate object
      "component_id": string, // exact component_id from the candidate object
      "score": float          // relevance score (0.0-1.0)
    }}
  ]
}}


Example:

Input:
User Query: "How does the payment processing component handle errors?"
Top-K: 2
Candidates:
  0: {{"filename": "billing.json", "component_id": "po_0"}}
  1: {{"filename": "auth.json", "component_id": "t_1"}}
  2: {{"filename": "billing.json", "component_id": "i_10"}}

Output:
{{
  "ranking": [
    {{"index": 0, "filename": "billing.json", "component_id": "po_0", "score": 0.93}},
    {{"index": 2, "filename": "billing.json", "component_id": "i_10", "score": 0.71}}
  ]
}}

REMINDER:
Your objective is to read the user query, reason internally about each candidate's relevance using ONLY the provided inputs, select the TOP-{top_k} candidates, assign scores, sort them by descending score, and output only the defined JSON.

Inputs:
# User Query
{query}

# Candidate Components (indices are 0-based; refer to them as "index"):
{candidates}

# Top-K
{top_k}

Output:
\end{promptbox}