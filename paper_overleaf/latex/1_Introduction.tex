\section{Introduction}\label{sec:intro}



\begin{figure}[ht]
  \centering
  \includegraphics[width=\linewidth]{figures/introduction_motivation.pdf}
  \caption{Challenges of TextRAG approaches and VisRAG approaches.
    (a) Incorrect summarization may result in possible information loss in TextRAG.
    (b) Insufficient retrieval granularity in VisRAG.
    (c) Limited multihop reasoning due to loss of links in VisRAG.
  }
  \label{fig:introduction_motivation}
  \vspace{-0.5cm}
\end{figure}



Multimodal retrieval is a rapidly advancing research area, crucial for enhancing modern information retrieval systems~\cite{blip, blip2, clip}. 
Early studies primarily focused on multimodal component retrieval, where components such as text, tables, and images had limited or no explicit relationships~\cite{multimodalqa, webqa}. 
Recently, however, there has been an emerging shift toward open-domain multimodal document retrieval, where closely related components of various modalities are grouped together as a unified document, such as webpages or PDFs~\cite{visrag, m3docrag}.
Such multimodal documents can be viewed as collections of potentially interconnected components (e.g., via hyperlinks as shown with \texttt{Taj Mahal} in Figure~\ref{fig:introduction_motivation}), each belonging to one of multiple modalities, including text, tables, or images.

Recent approaches in multimodal document retrieval have increasingly adopted \textit{VisRAG}-based methodologies, which unify diverse modalities by treating them primarily as visual content, typically represented through screenshots such as a page of a PDF file~\cite{visrag, colpali, m3docrag}. 
By casting multimodal retrieval as essentially an image retrieval problem, these methods leverage advanced vision-based embedding models to preserve multimodal information. 

This paradigm emerged largely as a response to the limitations of earlier \textit{TextRAG}-based approaches, which predominantly relied on textual retrieval by converting visual data into textual summaries~\cite{textrag1, textrag2, textrag3, awakening, skurg, solar, unimmqa}. 
Although effective in leveraging mature text retrieval systems, these methods inherently struggled to represent visual content adequately, resulting in potential information loss and reduction in retrieval effectiveness.
For example, in Figure~\ref{fig:introduction_motivation}, the summary of the \texttt{Taj Mahal}'s image omits the word \texttt{minarets}, which was crucial for answering the query in this context.


Despite their conceptual advances, current multimodal retrieval approaches, including VisRAG, still face two crucial limitations:

\textbf{(1) Insufficient consideration of retrieval granularity.}
Effective retrieval demands explicitly setting an optimal granularity of information representation ~\cite{densexretrieval}. 
Existing VisRAG methods, however, typically adopt a fixed, single-granular approach---generally at the full-page screenshot level---which may include multiple components irrelevant to the query. 
Empirically, we observed that a single screenshot typically comprises an average of three distinct components.
Consequently, the portion of query-relevant information within each screenshot is relatively small, inevitably leading to diminished embedding quality and retrieval effectiveness.
Thus, granularity-aware retrieval remains largely unaddressed within multimodal document retrieval settings.
For example, in Figure~\ref{fig:introduction_motivation}(b), VisRAG struggles because the query-relevant information constitutes only a small portion of the screenshot's content.

\textbf{(2) Limited capability for multihop reasoning.}
Multimodal document retrieval inherently requires reasoning about complex intra- and inter-document relationships among components.
Effective multihop reasoning critically depends on capturing these relationships, as within-document retrieval often necessitates integrating complementary information distributed across multiple modalities to fully represent an entity.
Likewise, inter-document retrieval typically demands traversing semantic connections between related documents. 
Existing VisRAG-based approaches, however, independently embed and retrieve individual screenshots via nearest-neighbor search, thereby overlooking essential interdependencies among components.
Moreover, these methods disregard inherent structural connections within the same document, such as associations among screenshots originating from the same page or hyperlinks explicitly linking different components.
Although some multimodal component retrieval methods have introduced multihop reasoning capabilities~\cite{skurg}, they largely focus on distractor-based closed-domain settings and rely heavily on online reasoning with Large Language Models, significantly limiting their generalization to open-domain multimodal document retrieval scenarios.
For instance, in Figure~\ref{fig:introduction_motivation}(c), VisRAG struggles with multihop reasoning because it does not utilize the structural link from \texttt{Shah Jahan} to \texttt{Taj Mahal}.






To address the challenges, we propose \texttt{LILaC}, an effective multimodal retrieval approach with two novel ideas:

\textbf{(1) Layered component graph construction.}
We first represent the multimodal document corpus as a layered component graph, explicitly designed to capture multimodal information at two distinct granularities. 
This layered graph structure leverages edges to explicitly encode relationships among components within and across documents, thus inherently facilitating effective multihop reasoning.
Additionally, we utilize a layered representation, enhancing retrieval efficiency and effectiveness.
The coarse-grained layer---where textual content is represented as paragraphs, tables as whole entities, and images in their entirety---provides contextual understanding suitable for broad candidate generation.
While in the fine-grained layer---where paragraphs are extracted into  sentences, tables into discrete rows, and images into detected visual objects---enables precise reasoning by decomposing content into finer units. 
Edges in the coarse-grained layer capture semantic associations among components, while edges connecting coarse-grained nodes to their fine-grained subcomponents represent hierarchical containment relationships.

\textbf{(2) Late-interaction-based subgraph retrieval in layered graph.}
At online time, \texttt{LILaC} retrieves a query-relevant subgraph from the layered component graph. 
A key challenge in this step is the combinatorial explosion of candidate subgraphs, resulting from the extensive number of nodes and edges distributed across both granularity layers~\cite{grag}.
To efficiently manage this complexity, we propose a traversal-based subgraph retrieval method on the layered component graph.
Specifically, we first decompose the original query to identify an initial candidate node set at the coarse-grained layer.
We then iteratively perform beam search by traversing connected edges from these initial candidates, dynamically computing relevance scores at each step.
Crucially, since explicitly computing scores for all potential edges would be computationally prohibitive, we leverage the layered structure of both the graph and query decomposition.
In particular, edge scores are computed dynamically via late interaction between the fine-grained subqueries and the fine-grained nodes associated with each candidate edge, effectively utilizing node-level embeddings. 






In summary, we make three key contributions: 
(1)  We introduce a layered graph structure capturing multimodal documents at dual granularities, effectively supporting multihop reasoning.
(2) We propose an efficient yet effective subgraph retrieval method leveraging late interaction between decomposed queries and fine-grained components.
\updated{(3) Extensive experiments demonstrate that our approach achieves state-of-the-art retrieval accuracy on all five benchmarks, notably using only pretrained models without additional fine-tuning.}











