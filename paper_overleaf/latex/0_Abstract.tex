\begin{abstract}

Multimodal document retrieval aims to retrieve query-relevant components from documents composed of textual, tabular, and visual elements. 
An effective multimodal retriever needs to handle two main challenges:
(1) mitigate the effect of irrelevant contents caused by fixed, single-granular retrieval units, and 
(2) support multihop reasoning by effectively capturing semantic relationships among components within and across documents. 
To address these challenges, we propose \texttt{LILaC}, a multimodal retrieval framework featuring two core innovations. 
First, we introduce a \textit{layered component graph}, explicitly representing multimodal information at two layers---each representing coarse and fine granularity---facilitating efficient yet precise reasoning. 
Second, we develop a \textit{late-interaction-based subgraph retrieval} method, an edge-based approach that initially identifies coarse-grained nodes for efficient candidate generation, then performs fine-grained reasoning via late interaction.
\updated{Extensive experiments demonstrate that \texttt{LILaC} achieves state-of-the-art retrieval performance on all five benchmarks, notably without additional fine-tuning.
We make the artifacts publicly available at \href{https://github.com/joohyung00/lilac}{\textcolor{linkpink}{\texttt{github.com/joohyung00/lilac}}}.
}
\end{abstract}

