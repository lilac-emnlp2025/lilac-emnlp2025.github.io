@article{visrag,
  title={Visrag: Vision-based retrieval-augmented generation on multi-modality documents},
  author={Yu, Shi and Tang, Chaoyue and Xu, Bokai and Cui, Junbo and Ran, Junhao and Yan, Yukun and Liu, Zhenghao and Wang, Shuo and Han, Xu and Liu, Zhiyuan and others},
  journal={arXiv preprint arXiv:2410.10594},
  year={2024}
}

@inproceedings{colpali,
  title={Colpali: Efficient document retrieval with vision language models},
  author={Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, C{\'e}line and Colombo, Pierre},
  booktitle={The Thirteenth International Conference on Learning Representations},
  year={2024}
}

@article{m3docrag,
  title={M3docrag: Multi-modal retrieval is what you need for multi-page multi-document understanding},
  author={Cho, Jaemin and Mahata, Debanjan and Irsoy, Ozan and He, Yujie and Bansal, Mohit},
  journal={arXiv preprint arXiv:2411.04952},
  year={2024}
}








@article{multimodalqa,
  title={Multimodalqa: Complex question answering over text, tables and images},
  author={Talmor, Alon and Yoran, Ori and Catav, Amnon and Lahav, Dan and Wang, Yizhong and Asai, Akari and Ilharco, Gabriel and Hajishirzi, Hannaneh and Berant, Jonathan},
  journal={arXiv preprint arXiv:2104.06039},
  year={2021}
}

@inproceedings{mmcoqa,
  title={Mmcoqa: Conversational question answering over text, tables, and images},
  author={Li, Yongqi and Li, Wenjie and Nie, Liqiang},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={4220--4231},
  year={2022}
}

@article{mpdocvqa,
  title={Hierarchical multimodal transformers for multipage docvqa},
  author={Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest},
  journal={Pattern Recognition},
  volume={144},
  pages={109834},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{infovqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1697--1706},
  year={2022}
}

@inproceedings{slidevqa,
  title={Slidevqa: A dataset for document visual question answering on multiple images},
  author={Tanaka, Ryota and Nishida, Kyosuke and Nishida, Kosuke and Hasegawa, Taku and Saito, Itsumi and Saito, Kuniko},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={11},
  pages={13636--13645},
  year={2023}
}

@inproceedings{webqa,
  title={Webqa: Multihop and multimodal qa},
  author={Chang, Yingshan and Narang, Mridu and Suzuki, Hisami and Cao, Guihong and Gao, Jianfeng and Bisk, Yonatan},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16495--16504},
  year={2022}
}






@inproceedings{colbert,
  title={Colbert: Efficient and effective passage search via contextualized late interaction over bert},
  author={Khattab, Omar and Zaharia, Matei},
  booktitle={Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval},
  pages={39--48},
  year={2020}
}

@article{colbertv2,
  title={Colbertv2: Effective and efficient retrieval via lightweight late interaction},
  author={Santhanam, Keshav and Khattab, Omar and Saad-Falcon, Jon and Potts, Christopher and Zaharia, Matei},
  journal={arXiv preprint arXiv:2112.01488},
  year={2021}
}





@article{yolo_dla,
  title={Doclayout-yolo: Enhancing document layout analysis through diverse synthetic data and global-to-local adaptive perception},
  author={Zhao, Zhiyuan and Kang, Hengrui and Wang, Bin and He, Conghui},
  journal={arXiv preprint arXiv:2410.12628},
  year={2024}
}

@article{yolov10,
  title={Yolov10: Real-time end-to-end object detection},
  author={Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={107984--108011},
  year={2024}
}

@article{sat,
  title={Segment Any Text: A universal approach for robust, efficient and adaptable sentence segmentation},
  author={Frohmann, Markus and Sterner, Igor and Vuli{\'c}, Ivan and Minixhofer, Benjamin and Schedl, Markus},
  journal={arXiv preprint arXiv:2406.16678},
  year={2024}
}





@inproceedings{raptor,
  title={Raptor: Recursive abstractive processing for tree-organized retrieval},
  author={Sarthi, Parth and Abdullah, Salman and Tuli, Aditi and Khanna, Shubh and Goldie, Anna and Manning, Christopher D},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{densexretrieval,
  title={Dense x retrieval: What retrieval granularity should we use?},
  author={Chen, Tong and Wang, Hongwei and Chen, Sihao and Yu, Wenhao and Ma, Kaixin and Zhao, Xinran and Zhang, Hongming and Yu, Dong},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={15159--15177},
  year={2024}
}

@article{mixofgranularity,
  title={Mix-of-granularity: Optimize the chunking granularity for retrieval-augmented generation},
  author={Zhong, Zijie and Liu, Hanwen and Cui, Xiaoya and Zhang, Xiaofan and Qin, Zengchang},
  journal={arXiv preprint arXiv:2406.00456},
  year={2024}
}

@article{financialchunking,
  title={Financial report chunking for effective retrieval augmented generation},
  author={Yepes, Antonio Jimeno and You, Yao and Milczek, Jan and Laverde, Sebastian and Li, Renyu},
  journal={arXiv preprint arXiv:2402.05131},
  year={2024}
}





@article{decomposition1,
  title={Multi-hop reading comprehension through question decomposition and rescoring},
  author={Min, Sewon and Zhong, Victor and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:1906.02916},
  year={2019}
}

@article{decomposition2,
  title={Coarse-grained decomposition and fine-grained interaction for multi-hop question answering},
  author={Cao, Xing and Liu, Yun},
  journal={Journal of Intelligent Information Systems},
  volume={58},
  number={1},
  pages={21--41},
  year={2022},
  publisher={Springer}
}

@article{decomposition3,
  title={GenDec: A robust generative Question-decomposition method for Multi-hop reasoning},
  author={Wu, Jian and Yang, Linyi and Ji, Yuliang and Huang, Wenhao and Karlsson, B{\"o}rje F and Okumura, Manabu},
  journal={arXiv preprint arXiv:2402.11166},
  year={2024}
}




@article{hierarchygraphtext,
  title={Hierarchical graph network for multi-hop question answering},
  author={Fang, Yuwei and Sun, Siqi and Gan, Zhe and Pillai, Rohit and Wang, Shuohang and Liu, Jingjing},
  journal={arXiv preprint arXiv:1911.03631},
  year={2019}
}





@inproceedings{helios,
  title={HELIOS: Harmonizing Early Fusion, Late Fusion, and LLM Reasoning for Multi-Granular Table-Text Retrieval},
  author={Park, Sungho and Yun, Joohyung and Lee, Jongwuk and Han, Wook-Shin},
  booktitle={Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={32424--32444},
  year={2025}
}

@article{ottqa,
  title={Open domain question answering over tables via dense retrieval},
  author={Herzig, Jonathan and M{\"u}ller, Thomas and Krichene, Syrine and Eisenschlos, Julian Martin},
  journal={arXiv preprint arXiv:2103.12011},
  year={2021}
}

@article{otter,
  title={Mixed-modality representation learning and pre-training for joint table-and-text retrieval in openqa},
  author={Huang, Junjie and Zhong, Wanjun and Liu, Qian and Gong, Ming and Jiang, Daxin and Duan, Nan},
  journal={arXiv preprint arXiv:2210.05197},
  year={2022}
}

@article{dotter,
  title={Denoising Table-Text Retrieval for Open-Domain Question Answering},
  author={Kang, Deokhyung and Jung, Baikjin and Kim, Yunsu and Lee, Gary Geunbae},
  journal={arXiv preprint arXiv:2403.17611},
  year={2024}
}








@article{mmragsurvey,
  title={A Survey of Multimodal Retrieval-Augmented Generation},
  author={Mei, Lang and Mo, Siyu and Yang, Zhihan and Chen, Chong},
  journal={arXiv preprint arXiv:2504.08748},
  year={2025}
}

@article{murag,
  title={Murag: Multimodal retrieval-augmented generator for open question answering over images and text},
  author={Chen, Wenhu and Hu, Hexiang and Chen, Xi and Verga, Pat and Cohen, William W},
  journal={arXiv preprint arXiv:2210.02928},
  year={2022}
}

@inproceedings{skurg,
  title={Enhancing multi-modal multi-hop question answering via structured knowledge and unified retrieval-generation},
  author={Yang, Qian and Chen, Qian and Wang, Wen and Hu, Baotian and Zhang, Min},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={5223--5234},
  year={2023}
}

@article{solar,
  title={Unified language representation for question answering over text, tables, and images},
  author={Yu, Bowen and Fu, Cheng and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  journal={arXiv preprint arXiv:2306.16762},
  year={2023}
}

@inproceedings{unimmqa,
  title={Unifying text, tables, and images for multimodal question answering},
  author={Luo, Haohao and Shen, Ying and Deng, Yang},
  year={2023},
  organization={Association for Computational Linguistics}
}

@inproceedings{mmentail,
  title={An Entailment Tree Generation Approach for Multimodal Multi-Hop Question Answering with Mixture-of-Experts and Iterative Feedback Mechanism},
  author={Zhang, Qing and Lv, Haocheng and Liu, Jie and Chen, Zhiyun and Duan, Jianyong and Wang, Hao and He, Li and Xu, Mingying},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={4814--4822},
  year={2024}
}

@article{perqa,
  title={Progressive evidence refinement for open-domain multimodal retrieval question answering},
  author={Yang, Shuwen and Wu, Anran and Wu, Xingjiao and Xiao, Luwei and Ma, Tianlong and Jin, Cheng and He, Liang},
  journal={arXiv preprint arXiv:2310.09696},
  year={2023}
}

@article{beyondtext,
  title={Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications},
  author={Riedler, Monica and Langer, Stefan},
  journal={arXiv preprint arXiv:2410.21943},
  year={2024}
}








@inproceedings{uniir,
  title={Uniir: Training and benchmarking universal multimodal information retrievers},
  author={Wei, Cong and Chen, Yang and Chen, Haonan and Hu, Hexiang and Zhang, Ge and Fu, Jie and Ritter, Alan and Chen, Wenhu},
  booktitle={European Conference on Computer Vision},
  pages={387--404},
  year={2024},
  organization={Springer}
}

@article{vlm2vec,
  title={Vlm2vec: Training vision-language models for massive multimodal embedding tasks},
  author={Jiang, Ziyan and Meng, Rui and Yang, Xinyi and Yavuz, Semih and Zhou, Yingbo and Chen, Wenhu},
  journal={arXiv preprint arXiv:2410.05160},
  year={2024}
}

@article{mmembed,
  title={Mm-embed: Universal multimodal retrieval with multimodal llms},
  author={Lin, Sheng-Chieh and Lee, Chankyu and Shoeybi, Mohammad and Lin, Jimmy and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2411.02571},
  year={2024}
}

@article{nvembedv2,
  title={Nv-embed: Improved techniques for training llms as generalist embedding models},
  author={Lee, Chankyu and Roy, Rajarshi and Xu, Mengyao and Raiman, Jonathan and Shoeybi, Mohammad and Catanzaro, Bryan and Ping, Wei},
  journal={arXiv preprint arXiv:2405.17428},
  year={2024}
}

@article{jasper,
  title={Jasper and Stella: distillation of SOTA embedding models},
  author={Zhang, Dun and Li, Jiacheng and Zeng, Ziyang and Wang, Fulong},
  journal={arXiv preprint arXiv:2412.19048},
  year={2024}
}

@article{unime,
  title={Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs},
  author={Gu, Tiancheng and Yang, Kaicheng and Feng, Ziyong and Wang, Xingjun and Zhang, Yanzhao and Long, Dingkun and Chen, Yingda and Cai, Weidong and Deng, Jiankang},
  journal={arXiv preprint arXiv:2504.17432},
  year={2025}
}

@article{llave,
  title={LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning},
  author={Lan, Zhibin and Niu, Liqiang and Meng, Fandong and Zhou, Jie and Su, Jinsong},
  journal={arXiv preprint arXiv:2503.04812},
  year={2025}
}

@article{mme5,
  title={mmE5: Improving Multimodal Multilingual Embeddings via High-quality Synthetic Data},
  author={Chen, Haonan and Wang, Liang and Yang, Nan and Zhu, Yutao and Zhao, Ziliang and Wei, Furu and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2502.08468},
  year={2025}
}

@article{e5v,
  title={E5-v: Universal embeddings with multimodal large language models},
  author={Jiang, Ting and Song, Minghui and Zhang, Zihan and Huang, Haizhen and Deng, Weiwei and Sun, Feng and Zhang, Qi and Wang, Deqing and Zhuang, Fuzhen},
  journal={arXiv preprint arXiv:2407.12580},
  year={2024}
}








@article{mteb,
  title={MTEB: Massive text embedding benchmark},
  author={Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"\i}c and Reimers, Nils},
  journal={arXiv preprint arXiv:2210.07316},
  year={2022}
}

@inproceedings{dpr,
  title={Dense Passage Retrieval for Open-Domain Question Answering.},
  author={Karpukhin, Vladimir and Oguz, Barlas and Min, Sewon and Lewis, Patrick SH and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  booktitle={EMNLP (1)},
  pages={6769--6781},
  year={2020}
}

@article{sentencebert,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@inproceedings{clip,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

@inproceedings{blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@article{blip2,
  title={Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing},
  author={Li, Dongxu and Li, Junnan and Hoi, Steven},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={30146--30166},
  year={2023}
}

@article{preflmr,
  title={Preflmr: Scaling up fine-grained late-interaction multi-modal retrievers},
  author={Lin, Weizhe and Mei, Jingbiao and Chen, Jinghong and Byrne, Bill},
  journal={arXiv preprint arXiv:2402.08327},
  year={2024}
}


@article{grag,
  title={Grag: Graph retrieval-augmented generation},
  author={Hu, Yuntong and Lei, Zhihan and Zhang, Zheng and Pan, Bo and Ling, Chen and Zhao, Liang},
  journal={arXiv preprint arXiv:2405.16506},
  year={2024}
}




@article{qwen2_vl,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}

@article{qwen2_5_vl,
  title={Qwen2. 5-vl technical report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{qwen2_5,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{minicpm,
  title={Minicpm-v: A gpt-4v level mllm on your phone},
  author={Yao, Yuan and Yu, Tianyu and Zhang, Ao and Wang, Chongyi and Cui, Junbo and Zhu, Hongji and Cai, Tianchi and Li, Haoyu and Zhao, Weilin and He, Zhihui and others},
  journal={arXiv preprint arXiv:2408.01800},
  year={2024}
}





@article{textrag1,
  title={Augmentation-adapted retriever improves generalization of language models as generic plug-in},
  author={Yu, Zichun and Xiong, Chenyan and Yu, Shi and Liu, Zhiyuan},
  journal={arXiv preprint arXiv:2305.17331},
  year={2023}
}

@inproceedings{textrag2,
  title={Self-rag: Learning to retrieve, generate, and critique through self-reflection},
  author={Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{textrag3,
  title={Corrective retrieval augmented generation},
  author={Yan, Shi-Qi and Gu, Jia-Chen and Zhu, Yun and Ling, Zhen-Hua},
  year={2024}
}

@article{hallucination1,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM computing surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@article{hallucination2,
  title={A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity},
  author={Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and others},
  journal={arXiv preprint arXiv:2302.04023},
  year={2023}
}





@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}

@article{awakening,
  title={Awakening LLMs' Reasoning Potential: A Fine-Grained Pipeline to Evaluate and Mitigate Vague Perception},
  author={Ling, Zipeng and Tang, Yuehao and Liu, Shuliang and Yang, Junqi and Fu, Shenghong and Huang, Chen and Huang, Kejia and Wan, Yao and Hou, Zhichao and Hu, Xuming},
  journal={arXiv preprint arXiv:2507.16199},
  year={2025}
}